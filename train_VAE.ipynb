{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lbinding/AT-AT/blob/main/train_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install\n",
        "!pip install monai==1.4.0\n",
        "!pip install pandas==2.0.3\n",
        "!pip install torchio==0.20.4"
      ],
      "metadata": {
        "id": "oCSim790cePy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a0031b6-230c-4f7e-a4e0-b6a7db951177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: monai==1.4.0 in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.24 in /usr/local/lib/python3.11/dist-packages (from monai==1.4.0) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from monai==1.4.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.4.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9->monai==1.4.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9->monai==1.4.0) (3.0.2)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2025.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (1.26.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.17.0)\n",
            "Requirement already satisfied: torchio==0.20.4 in /usr/local/lib/python3.11/dist-packages (0.20.4)\n",
            "Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.11/dist-packages (from torchio==0.20.4) (1.2.18)\n",
            "Requirement already satisfied: humanize>=0.1 in /usr/local/lib/python3.11/dist-packages (from torchio==0.20.4) (4.12.3)\n",
            "Requirement already satisfied: nibabel>=3 in /usr/local/lib/python3.11/dist-packages (from torchio==0.20.4) (5.3.2)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from torchio==0.20.4) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from torchio==0.20.4) (24.2)\n",
            "Requirement already satisfied: rich>=10 in /usr/local/lib/python3.11/dist-packages (from torchio==0.20.4) (13.9.4)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.11/dist-packages (from torchio==0.20.4) (1.15.3)\n",
            "Requirement already satisfied: simpleitk!=2.0.*,!=2.1.1.1,>=1.3 in /usr/local/lib/python3.11/dist-packages (from torchio==0.20.4) (2.5.2)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from torchio==0.20.4) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.40 in /usr/local/lib/python3.11/dist-packages (from torchio==0.20.4) (4.67.1)\n",
            "Requirement already satisfied: typer>=0.1 in /usr/local/lib/python3.11/dist-packages (from torchio==0.20.4) (0.16.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2->torchio==0.20.4) (1.17.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel>=3->torchio==0.20.4) (6.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel>=3->torchio==0.20.4) (4.14.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10->torchio==0.20.4) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10->torchio==0.20.4) (2.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio==0.20.4) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9->torchio==0.20.4) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.1->torchio==0.20.4) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.1->torchio==0.20.4) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10->torchio==0.20.4) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9->torchio==0.20.4) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Libs\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "import torchio as tio\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import nibabel as nib\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from monai.bundle import ConfigParser\n",
        "from monai.losses import PerceptualLoss\n",
        "from monai.losses.adversarial_loss import PatchAdversarialLoss\n",
        "from torch.amp import autocast\n",
        "import wandb\n",
        "from torch.nn import MSELoss\n",
        "from monai.transforms import DivisiblePad\n",
        "import os\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "rFNCCEEJbP-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Setup drive path\n",
        "from google.colab import drive\n",
        "base_dir = '/content/drive'\n",
        "drive.mount(base_dir)\n",
        "base_dir = '/content/drive/MyDrive'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E36pf_c_gniU",
        "outputId": "23c5b613-0f65-4086-8846-d3473df1ab84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_folder_path_in_drive = os.path.join(base_dir, \"training_data\") # <--- CHANGE THIS PATH\n",
        "\n",
        "# The destination path in the local Colab runtime.\n",
        "# '/content/' is a common place for temporary files in Colab.\n",
        "# You can give your copied folder a new name here if you want.\n",
        "destination_folder_path_local = '/content/' # <--- OPTIONAL: Change the name of the copied folder\n",
        "\n",
        "print(f\"Source folder in Drive: '{source_folder_path_in_drive}'\")\n",
        "print(f\"Destination in local Colab runtime: '{destination_folder_path_local}'\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- Step 3: Check if the source folder exists ---\n",
        "if not os.path.exists(source_folder_path_in_drive):\n",
        "    print(f\"Error: The source folder '{source_folder_path_in_drive}' does not exist in your Google Drive.\")\n",
        "    print(\"Please double-check the 'source_folder_path_in_drive' variable.\")\n",
        "else:\n",
        "    # --- Step 4: Create the destination directory if it doesn't exist ---\n",
        "    # This is good practice to ensure the target path is ready.\n",
        "    os.makedirs(destination_folder_path_local, exist_ok=True)\n",
        "    print(f\"Ensured destination directory exists: '{destination_folder_path_local}'\")\n",
        "\n",
        "    # --- Step 5: Copy the folder using the 'cp' command ---\n",
        "    # `cp -r` recursively copies directories and their contents.\n",
        "    print(f\"Copying folder from Drive to local Colab runtime...\")\n",
        "    !cp -r \"$source_folder_path_in_drive\" \"$destination_folder_path_local\"\n",
        "\n",
        "    print(\"Copy process complete!\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # --- Step 6: Verify the copy (optional) ---\n",
        "    print(f\"Listing contents of the copied folder in local Colab runspace:\")\n",
        "    !ls -l \"$destination_folder_path_local\"\n",
        "\n",
        "    print(\"\\nYou can now work with the copied folder at:\")\n",
        "    print(destination_folder_path_local)\n",
        "    print(\"Files accessed from this location will typically be faster than directly from Google Drive.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFUo_jNVr39S",
        "outputId": "af509d35-e63f-4f69-8e5f-626836e6e278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source folder in Drive: '/content/drive/MyDrive/training_data'\n",
            "Destination in local Colab runtime: '/content/'\n",
            "--------------------------------------------------\n",
            "Ensured destination directory exists: '/content/'\n",
            "Copying folder from Drive to local Colab runtime...\n",
            "Copy process complete!\n",
            "--------------------------------------------------\n",
            "Listing contents of the copied folder in local Colab runspace:\n",
            "total 12\n",
            "drwx------ 5 root root 4096 Jul  2 15:24 drive\n",
            "drwxr-xr-x 1 root root 4096 Jun 26 13:35 sample_data\n",
            "drwx------ 3 root root 4096 Jul  2 15:24 training_data\n",
            "\n",
            "You can now work with the copied folder at:\n",
            "/content/\n",
            "Files accessed from this location will typically be faster than directly from Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVE MODEL\n"
      ],
      "metadata": {
        "id": "_znyOosUgUdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(vAE=None, LDM=None, discrim=None, model_dir=None, epoch=None):\n",
        "    \"\"\"\n",
        "    Save the model state dictionary to a specified directory with epoch information.\n",
        "\n",
        "    Args:\n",
        "        vAE (torch.nn.Module, optional): The Variational Autoencoder model to save.\n",
        "        LDM (torch.nn.Module, optional): The Latent Diffusion Model to save.\n",
        "        model_dir (str): Directory where the models will be saved.\n",
        "        epoch (int): Current epoch number for naming the file.\n",
        "    \"\"\"\n",
        "    if model_dir is None or epoch is None:\n",
        "        raise ValueError(\"Both 'model_dir' and 'epoch' must be provided.\")\n",
        "\n",
        "    if vAE is not None:\n",
        "        model_save_path = os.path.join(model_dir, \"models\", f\"trained_vAE_epoch_{epoch}.pt\")\n",
        "        torch.save(vAE.state_dict(), model_save_path)\n",
        "        print(f\"vAE model saved at {model_save_path}\")\n",
        "\n",
        "    if LDM is not None:\n",
        "        model_save_path = os.path.join(model_dir, \"models\", f\"trained_LDM_epoch_{epoch}.pt\")\n",
        "        torch.save(LDM.state_dict(), model_save_path)\n",
        "        print(f\"LDM model saved at {model_save_path}\")\n",
        "\n",
        "    if discrim is not None:\n",
        "        discrim_save_path = os.path.join(model_dir, \"models\", f\"trained_discriminator_epoch_{epoch}.pt\")\n",
        "        torch.save(discrim.state_dict(), discrim_save_path)\n",
        "        print(f\"Discriminator saved at {discrim_save_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "JuwjLN-JgWIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASET\n"
      ],
      "metadata": {
        "id": "dOgLKxoLbSeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Dataset\n",
        "\n",
        "class FDG_Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset to load NIfTI files from a provided list of file paths.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            file_paths (list of Path objects): List of paths to the NIfTI files.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "\n",
        "        self.T1_paths = data['Linked_Files_Anon']\n",
        "        self.diagnosis = data['DX_encoded']\n",
        "        self.transform = transform\n",
        "        self.rescale = tio.RescaleIntensity((0, 1))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.T1_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Get path to load in\n",
        "        T1_path = self.T1_paths[idx]\n",
        "\n",
        "        diag = self.diagnosis[idx]\n",
        "\n",
        "        # Load the T1-weighted MRI image\n",
        "        # Using memmap=False can prevent potential file locking issues\n",
        "        T1_img = nib.load(T1_path).get_fdata()\n",
        "\n",
        "        # Wrap in Subject (Add dimensions for channels and 3D (for augmentations))\n",
        "        subject = tio.Subject(image=tio.ScalarImage(tensor=torch.as_tensor(T1_img[None, :, :, None])))\n",
        "\n",
        "        # Apply augmentation\n",
        "        if self.transform:\n",
        "            subject = self.transform(subject)\n",
        "        else:\n",
        "            subject = self.rescale(subject)\n",
        "\n",
        "        # Extract transformed image tensor\n",
        "        image_tensor = subject['image']['data'].squeeze(-1) # Squeeze the dummy depth dimension\n",
        "\n",
        "        return image_tensor, diag\n",
        "\n",
        "def create_datasets(df, augmentations, data_dir):\n",
        "\n",
        "    df['Linked_Files_Anon'] = df['Linked_Files_Anon'].apply(lambda x: os.path.join(data_dir, x))\n",
        "\n",
        "    train_data = df[df['Set']=='Train']\n",
        "    valid_data = df[df['Set']=='Validation']\n",
        "    test_data = df[df['Set']=='Test']\n",
        "\n",
        "    train_data = train_data.reset_index(drop=True)\n",
        "    valid_data = valid_data.reset_index(drop=True)\n",
        "    test_data = test_data.reset_index(drop=True)\n",
        "\n",
        "    train_dataset = FDG_Dataset(data=train_data, transform=augmentations)\n",
        "    valid_dataset = FDG_Dataset(data=valid_data)\n",
        "    test_dataset = FDG_Dataset(data=test_data)\n",
        "\n",
        "\n",
        "\n",
        "    return train_dataset, valid_dataset, test_dataset"
      ],
      "metadata": {
        "id": "6furmEepbOzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD"
      ],
      "metadata": {
        "id": "cJsW7vsMbCpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_KL_autoencoder(weights_path, config_file=\"train_autoencoder.json\", device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Setup paths\n",
        "    config_path = os.path.join(base_dir, \"configs\", config_file)\n",
        "    #weights_path = base_dir / \"models\" / weights_file\n",
        "\n",
        "    print(config_path)\n",
        "\n",
        "    # Read config\n",
        "    config = ConfigParser()\n",
        "    config.read_config(str(config_path))\n",
        "\n",
        "    # Parse model\n",
        "    model = config.get_parsed_content(\"gnetwork\")\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(weights_path, map_location=device)\n",
        "\n",
        "    # Key remapping\n",
        "    key_mapping = {\n",
        "        \"encoder.blocks.10.to_q.weight\": \"encoder.blocks.10.attn.to_q.weight\",\n",
        "        \"encoder.blocks.10.to_q.bias\": \"encoder.blocks.10.attn.to_q.bias\",\n",
        "        \"encoder.blocks.10.to_k.weight\": \"encoder.blocks.10.attn.to_k.weight\",\n",
        "        \"encoder.blocks.10.to_k.bias\": \"encoder.blocks.10.attn.to_k.bias\",\n",
        "        \"encoder.blocks.10.to_v.weight\": \"encoder.blocks.10.attn.to_v.weight\",\n",
        "        \"encoder.blocks.10.to_v.bias\": \"encoder.blocks.10.attn.to_v.bias\",\n",
        "        \"encoder.blocks.10.proj_attn.weight\": \"encoder.blocks.10.attn.out_proj.weight\",\n",
        "        \"encoder.blocks.10.proj_attn.bias\": \"encoder.blocks.10.attn.out_proj.bias\",\n",
        "        \"decoder.blocks.2.to_q.weight\": \"decoder.blocks.2.attn.to_q.weight\",\n",
        "        \"decoder.blocks.2.to_q.bias\": \"decoder.blocks.2.attn.to_q.bias\",\n",
        "        \"decoder.blocks.2.to_k.weight\": \"decoder.blocks.2.attn.to_k.weight\",\n",
        "        \"decoder.blocks.2.to_k.bias\": \"decoder.blocks.2.attn.to_k.bias\",\n",
        "        \"decoder.blocks.2.to_v.weight\": \"decoder.blocks.2.attn.to_v.weight\",\n",
        "        \"decoder.blocks.2.to_v.bias\": \"decoder.blocks.2.attn.to_v.bias\",\n",
        "        \"decoder.blocks.2.proj_attn.weight\": \"decoder.blocks.2.attn.out_proj.weight\",\n",
        "        \"decoder.blocks.2.proj_attn.bias\": \"decoder.blocks.2.attn.out_proj.bias\",\n",
        "        \"decoder.blocks.6.conv.conv.weight\": \"decoder.blocks.6.postconv.conv.weight\",\n",
        "        \"decoder.blocks.6.conv.conv.bias\": \"decoder.blocks.6.postconv.conv.bias\",\n",
        "        \"decoder.blocks.9.conv.conv.weight\": \"decoder.blocks.9.postconv.conv.weight\",\n",
        "        \"decoder.blocks.9.conv.conv.bias\": \"decoder.blocks.9.postconv.conv.bias\",\n",
        "    }\n",
        "\n",
        "    # Remap keys\n",
        "    new_state_dict = {key_mapping.get(k, k): v for k, v in checkpoint.items()}\n",
        "\n",
        "    # Load state\n",
        "    model.load_state_dict(new_state_dict, strict=False)\n",
        "    model.to(device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "6d75iYr_bAbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TORCHIO"
      ],
      "metadata": {
        "id": "blr-IrH1bC4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Define the Augmentations class\n",
        "class Augmentations:\n",
        "    def __init__(self):\n",
        "        self.random_anisotropy = tio.RandomAnisotropy(axes=(0, 1))\n",
        "        self.random_affine = tio.RandomAffine()\n",
        "        self.add_motion = tio.RandomMotion(num_transforms=1, image_interpolation='nearest')\n",
        "        self.rescale = tio.RescaleIntensity((0, 1))\n",
        "\n",
        "    def __call__(self, subject):\n",
        "        aug_level = random.randint(1, 3)\n",
        "        # Define individual transformations\n",
        "        def blur(subject):\n",
        "            downsampling_factor = random.randint(2, 3)\n",
        "            original_spacing = 1 # This might need to be adjusted based on actual pixel spacing\n",
        "            std = tio.Resample.get_sigma(downsampling_factor, original_spacing)\n",
        "            antialiasing = tio.Blur(std) # Axes will default. Check if it's implicitly handling 2D correctly.\n",
        "            return antialiasing(subject)\n",
        "\n",
        "        def anistropy(subject):\n",
        "            return self.random_anisotropy(subject)\n",
        "\n",
        "        def affine(subject):\n",
        "            return self.random_affine(subject)\n",
        "\n",
        "        def elastix(subject):\n",
        "            max_displacement_value = random.randint(1, 5) # Still in voxels\n",
        "            # For 2D images, the axes should be (0, 1)\n",
        "            random_elastic = tio.RandomElasticDeformation(\n",
        "                max_displacement=max_displacement_value,\n",
        "                num_control_points=random.randint(5, 15),\n",
        "            )\n",
        "            return random_elastic(subject)\n",
        "\n",
        "        def noise(subject):\n",
        "            add_noise = tio.RandomNoise(std=(np.random.rand() / 4))\n",
        "            return add_noise(subject)\n",
        "\n",
        "        def field_bias(subject):\n",
        "            add_bias = tio.RandomBiasField(coefficients=(np.random.rand() / 2))\n",
        "            return add_bias(subject)\n",
        "\n",
        "        def motion(subject):\n",
        "            return self.add_motion(subject)\n",
        "\n",
        "        # List of functions\n",
        "        all_functions = [blur, anistropy, noise, field_bias, motion]\n",
        "        blur_functions = [noise, field_bias, anistropy]\n",
        "        other_functions = [motion, blur]\n",
        "\n",
        "        # Select transformations based on augmentation level\n",
        "        if aug_level == 0:\n",
        "            selected_functions = []\n",
        "        elif aug_level == 1:\n",
        "            selected_functions = random.sample(all_functions, 1)\n",
        "        elif aug_level == 2:\n",
        "            selected_blur_functions = random.sample(blur_functions, 1)\n",
        "            selected_other_functions = random.sample(other_functions, 1)\n",
        "            selected_functions = selected_blur_functions + selected_other_functions\n",
        "        elif aug_level == 3:\n",
        "            selected_blur_functions = random.sample(blur_functions, 2)\n",
        "            selected_other_functions = random.sample(other_functions, 2)\n",
        "            selected_functions = selected_blur_functions + selected_other_functions\n",
        "\n",
        "        # Apply transformations\n",
        "        subject = affine(subject)\n",
        "        subject = elastix(subject)\n",
        "\n",
        "        for func in selected_functions:\n",
        "            subject = func(subject)\n",
        "\n",
        "        return self.rescale(subject)\n"
      ],
      "metadata": {
        "id": "dJsRvRhabCMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOSSES"
      ],
      "metadata": {
        "id": "YILIjPwkbDLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Paths and Config\n",
        "weights_path = os.path.join(base_dir, \"models\", \"model_discriminator.pt\")\n",
        "config = ConfigParser()\n",
        "config.read_config(os.path.join(base_dir,\"configs\",\"train_autoencoder.json\"))\n",
        "\n",
        "#%% Weights for each component\n",
        "adv_weight = 0.5\n",
        "perceptual_weight = 1.0\n",
        "kl_weight = 1e-6  # KL regularization weight\n",
        "\n",
        "#%% Loss components\n",
        "intensity_loss = torch.nn.L1Loss()\n",
        "adv_loss = PatchAdversarialLoss(criterion=\"least_squares\")\n",
        "\n",
        "#%% KL divergence loss\n",
        "def compute_kl_loss(z_mu, z_sigma):\n",
        "    kl_loss = 0.5 * torch.sum(\n",
        "        z_mu.pow(2) + z_sigma.pow(2) - torch.log(z_sigma.pow(2)) - 1,\n",
        "        dim=list(range(1, len(z_sigma.shape)))\n",
        "    )\n",
        "    return torch.mean(kl_loss)\n",
        "\n",
        "#%% Perceptual loss (ResNet50)\n",
        "def load_perceptual_loss(device):\n",
        "    perceptual_loss = PerceptualLoss(\n",
        "        spatial_dims=2,\n",
        "        network_type=\"resnet50\",\n",
        "        pretrained=True,\n",
        "    )\n",
        "    perceptual_loss.to(device)\n",
        "    return perceptual_loss\n",
        "\n",
        "#%% Discriminator\n",
        "def load_discriminator(device):\n",
        "    discriminator = config.get_parsed_content(\"dnetwork\")\n",
        "    discriminator.to(device)\n",
        "    return discriminator\n",
        "\n",
        "#%% Generator loss function\n",
        "def generator_loss(gen_images, real_images, z_mu, z_sigma, disc_net, loss_perceptual, device):\n",
        "    with autocast(device_type=device.type, enabled=True):\n",
        "\n",
        "        recons_loss = intensity_loss(gen_images, real_images)\n",
        "        wandb.log({\"intensity loss\": recons_loss}) # Consider logging epoch averages instead\n",
        "\n",
        "        kl = compute_kl_loss(z_mu, z_sigma)\n",
        "        wandb.log({\"kl loss\": kl})\n",
        "\n",
        "        p_loss = loss_perceptual(gen_images, real_images)\n",
        "        wandb.log({\"perceptual loss\": p_loss})\n",
        "\n",
        "        # Base generator loss (reconstruction + KL + perceptual)\n",
        "        loss_g = recons_loss + kl_weight * kl + perceptual_weight * p_loss\n",
        "        wandb.log({\"gen base loss\": loss_g})\n",
        "\n",
        "        # Adversarial component\n",
        "        logits_fake = disc_net(gen_images)[-1]\n",
        "        gen_adv_loss = adv_loss(logits_fake, target_is_real=True, for_discriminator=False)\n",
        "        wandb.log({\"adversarial loss\": gen_adv_loss})\n",
        "\n",
        "        # --- FIX 1: Replaced in-place += with standard addition to fix the error ---\n",
        "        loss_g = loss_g + adv_weight * gen_adv_loss\n",
        "        # -------------------------------------------------------------------------\n",
        "        wandb.log({\"total generator loss\": loss_g})\n",
        "\n",
        "\n",
        "        # --- FIX 2 (Best Practice): Detach logits_fake for the discriminator's loss calculation ---\n",
        "        # This stops gradients from flowing back to the generator during the discriminator's update.\n",
        "        d_loss_fake = adv_loss(logits_fake.detach(), target_is_real=False, for_discriminator=True)\n",
        "        # ------------------------------------------------------------------------------------------\n",
        "        logits_real = disc_net(real_images.contiguous().detach())[-1]\n",
        "        d_loss_real = adv_loss(logits_real, target_is_real=True, for_discriminator=True)\n",
        "        discriminator_loss = (d_loss_fake + d_loss_real) * 0.5\n",
        "        loss_d = adv_weight * discriminator_loss\n",
        "        wandb.log({\"discriminator loss\": loss_d})\n",
        "\n",
        "\n",
        "    return loss_g, loss_d\n"
      ],
      "metadata": {
        "id": "p-ib4Q9FbB9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WANDB CONFIG"
      ],
      "metadata": {
        "id": "18EbOmTFbtGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_wandb_config_vAE():\n",
        "    \"\"\"\n",
        "    Load the Weights & Biases configuration\n",
        "\n",
        "    Returns:\n",
        "        dict: wandb config.\n",
        "    \"\"\"\n",
        "    wandb.login(key=\"76c124f9bfc89b958db96f3de53b29ddbfa1feb5\")\n",
        "\n",
        "    wandb_config = {\n",
        "        \"learning_rate\": 0.00005,\n",
        "        \"architecture\": \"Autoencoder_KL\",\n",
        "        \"dataset\": \"FDG_2D_slices\",\n",
        "        \"epochs\": 100,\n",
        "        \"batch_size\": 12,\n",
        "        }\n",
        "\n",
        "    wandb.init(\n",
        "        # set the wandb project where this run will be logged\n",
        "        project=\"LDM_FDG_vae\",\n",
        "        # track hyperparameters and run metadata\n",
        "        config=wandb_config,\n",
        "        mode=\"online\"  # Ensure wandb is properly initialized\n",
        "    )\n",
        "    return wandb_config\n"
      ],
      "metadata": {
        "id": "LE12o2Kwbs9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAIN AUTOENCODER"
      ],
      "metadata": {
        "id": "BSaJMPp3blua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intensity_loss = torch.nn.L1Loss()\n",
        "#%% Train vAE\n",
        "def train_autoencoder(KL_autoencoder, discriminator, perceptual_loss, generator_loss, optimizer_g, optimizer_d, train_loader, val_loader, output_dir, device):\n",
        "\n",
        "    wandb_config = load_wandb_config_vAE()\n",
        "    KL_autoencoder.train()\n",
        "\n",
        "    best_loss = np.inf\n",
        "\n",
        "    for epoch in range(wandb_config[\"epochs\"]):\n",
        "        print(\"EPOCH:\", epoch)\n",
        "        wandb.log({\"epoch\": epoch})\n",
        "        total_g_loss = 0\n",
        "        total_d_loss = 0\n",
        "\n",
        "        for data_augmented,_ in train_loader:\n",
        "            data_augmented = data_augmented.to(device).float()\n",
        "\n",
        "            optimizer_g.zero_grad()\n",
        "            optimizer_d.zero_grad()\n",
        "            with autocast(device_type='cuda', enabled=True):\n",
        "                recon, z_mu, z_sigma = KL_autoencoder(data_augmented)\n",
        "                gen_loss, disc_loss = generator_loss(recon, data_augmented, z_mu, z_sigma, discriminator, perceptual_loss, device)\n",
        "\n",
        "            gen_loss.backward(retain_graph=True)\n",
        "            optimizer_g.step()\n",
        "\n",
        "            disc_loss.backward()\n",
        "            optimizer_d.step()\n",
        "\n",
        "            total_g_loss += gen_loss.item()\n",
        "            total_d_loss += disc_loss.item()\n",
        "        # Save model every epoch\n",
        "\n",
        "        valid_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data,_ in val_loader:\n",
        "                with autocast(device_type='cuda', enabled=True):\n",
        "                    data = data.to(device).float()\n",
        "                    recon, _, _ = KL_autoencoder(data)\n",
        "\n",
        "                    recon_loss = intensity_loss(data, recon)\n",
        "\n",
        "                    valid_loss += recon_loss.item() # Accumulates only the Python number\n",
        "\n",
        "        epoch_valid_loss = valid_loss / len(val_loader)\n",
        "        wandb.log({'valid_loss': epoch_valid_loss})\n",
        "\n",
        "        if epoch_valid_loss < best_loss:\n",
        "            best_loss = epoch_valid_loss\n",
        "\n",
        "            save_model(vAE=KL_autoencoder, model_dir=output_dir, epoch=epoch)\n",
        "            save_model(discrim=discriminator, model_dir=output_dir, epoch=epoch)\n",
        "\n",
        "\n",
        "    return KL_autoencoder\n"
      ],
      "metadata": {
        "id": "aWtzyUlAblh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN CODE"
      ],
      "metadata": {
        "id": "QpGg3Tb-bArp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xlGoZDHayy6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "52d31dcf-c8b1-446a-ae0d-eb34df18a995"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>adversarial loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>discriminator loss</td><td>█▃▆▆▆▄▃▆▃▆▃▇▆▅▆▁▅▂▄▃▄▅▁█▃▄▅▄▄▅▂▅▃▅▆▆▆▄▅▄</td></tr><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>gen base loss</td><td>██▆▄▃▄▂▅▃▄▃▂▃▂▃▂▂▃▁▂▂▂▁▃▂▂▂▂▂▂▁▁▂▂▂▃▂▁▂▂</td></tr><tr><td>intensity loss</td><td>▃█▄▂▃▄▅█▃▄▂▅▄▄▂▄▃▃▃▁▃▃▁▃▅▂▃▂▁▂▁▂▄▂▂▂▁▂▂▃</td></tr><tr><td>kl loss</td><td>▃▃▅▄▄▅▆▃▄▃▅▅▄▂▄▅▃▄▃▃▃▆▃▆▃▃▆▆▃▅▅▄▅▅█▆▇▅▁▆</td></tr><tr><td>perceptual loss</td><td>█▆▅▄▅▃▂▃▂▂▃▂▂▂▃▂▂▁▂▂▁▂▃▂▂▁▁▂▁▂▃▂▂▂▂▁▁▁▂▁</td></tr><tr><td>total generator loss</td><td>█▃▄▁▄▄▄▃▃▃▂▃▂▂▂▃▃▂▂▁▂▂▁▂▂▂▂▁▂▁▁▁▁▂▁▁▂▂▂▁</td></tr><tr><td>valid_loss</td><td>▄▄▅█▁▂▂▄▂▁▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>adversarial loss</td><td>0.00081</td></tr><tr><td>discriminator loss</td><td>0.24963</td></tr><tr><td>epoch</td><td>14</td></tr><tr><td>gen base loss</td><td>0.25779</td></tr><tr><td>intensity loss</td><td>0.02166</td></tr><tr><td>kl loss</td><td>11566.87891</td></tr><tr><td>perceptual loss</td><td>0.22457</td></tr><tr><td>total generator loss</td><td>0.2582</td></tr><tr><td>valid_loss</td><td>0.14244</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">super-cherry-92</strong> at: <a href='https://wandb.ai/lbinding-ucl/LDM_FDG_vae/runs/spueeqd2' target=\"_blank\">https://wandb.ai/lbinding-ucl/LDM_FDG_vae/runs/spueeqd2</a><br> View project at: <a href='https://wandb.ai/lbinding-ucl/LDM_FDG_vae' target=\"_blank\">https://wandb.ai/lbinding-ucl/LDM_FDG_vae</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250702_153822-spueeqd2/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250702_162538-rx424gfc</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lbinding-ucl/LDM_FDG_vae/runs/rx424gfc' target=\"_blank\">genial-forest-93</a></strong> to <a href='https://wandb.ai/lbinding-ucl/LDM_FDG_vae' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/lbinding-ucl/LDM_FDG_vae' target=\"_blank\">https://wandb.ai/lbinding-ucl/LDM_FDG_vae</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/lbinding-ucl/LDM_FDG_vae/runs/rx424gfc' target=\"_blank\">https://wandb.ai/lbinding-ucl/LDM_FDG_vae/runs/rx424gfc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/configs/train_autoencoder.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training vAE\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">genial-forest-93</strong> at: <a href='https://wandb.ai/lbinding-ucl/LDM_FDG_vae/runs/rx424gfc' target=\"_blank\">https://wandb.ai/lbinding-ucl/LDM_FDG_vae/runs/rx424gfc</a><br> View project at: <a href='https://wandb.ai/lbinding-ucl/LDM_FDG_vae' target=\"_blank\">https://wandb.ai/lbinding-ucl/LDM_FDG_vae</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250702_162538-rx424gfc/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250702_162540-bj2lzif9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lbinding-ucl/LDM_FDG_vae/runs/bj2lzif9' target=\"_blank\">misty-cherry-94</a></strong> to <a href='https://wandb.ai/lbinding-ucl/LDM_FDG_vae' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/lbinding-ucl/LDM_FDG_vae' target=\"_blank\">https://wandb.ai/lbinding-ucl/LDM_FDG_vae</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/lbinding-ucl/LDM_FDG_vae/runs/bj2lzif9' target=\"_blank\">https://wandb.ai/lbinding-ucl/LDM_FDG_vae/runs/bj2lzif9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchio/data/image.py:248: UserWarning: Using TorchIO images without a torchio.SubjectsLoader in PyTorch >= 2.3 might have unexpected consequences, e.g., the collated batches will be instances of torchio.Subject with 5D images. Replace your PyTorch DataLoader with a torchio.SubjectsLoader so that the collated batch becomes a dictionary, as expected. See https://github.com/fepegar/torchio/issues/1179 for more context about this issue.\n",
            "  warnings.warn(message, stacklevel=1)\n",
            "/usr/local/lib/python3.11/dist-packages/torchio/transforms/augmentation/spatial/random_elastic_deformation.py:301: RuntimeWarning: The maximum displacement is larger than the coarse grid spacing for dimensions: [2], so folding may occur. Choose fewer control points or a smaller maximum displacement\n",
            "  self.parse_free_form_transform(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_0.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_0.pt\n",
            "EPOCH: 1\n",
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_1.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_1.pt\n",
            "EPOCH: 2\n",
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_2.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_2.pt\n",
            "EPOCH: 3\n",
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_3.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_3.pt\n",
            "EPOCH: 4\n",
            "EPOCH: 5\n",
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_5.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_5.pt\n",
            "EPOCH: 6\n",
            "EPOCH: 7\n",
            "EPOCH: 8\n",
            "EPOCH: 9\n",
            "EPOCH: 10\n",
            "EPOCH: 11\n",
            "EPOCH: 12\n",
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_12.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_12.pt\n",
            "EPOCH: 13\n",
            "EPOCH: 14\n",
            "EPOCH: 15\n",
            "EPOCH: 16\n",
            "EPOCH: 17\n",
            "EPOCH: 18\n",
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_18.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_18.pt\n",
            "EPOCH: 19\n",
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_19.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_19.pt\n",
            "EPOCH: 20\n",
            "EPOCH: 21\n",
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_21.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_21.pt\n",
            "EPOCH: 22\n",
            "EPOCH: 23\n",
            "EPOCH: 24\n",
            "EPOCH: 25\n",
            "EPOCH: 26\n",
            "EPOCH: 27\n",
            "EPOCH: 28\n",
            "EPOCH: 29\n",
            "EPOCH: 30\n",
            "EPOCH: 31\n",
            "EPOCH: 32\n",
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_32.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_32.pt\n",
            "EPOCH: 33\n",
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_33.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_33.pt\n",
            "EPOCH: 34\n",
            "EPOCH: 35\n",
            "EPOCH: 36\n",
            "EPOCH: 37\n",
            "EPOCH: 38\n",
            "EPOCH: 39\n",
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_39.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_39.pt\n",
            "EPOCH: 40\n",
            "EPOCH: 41\n",
            "EPOCH: 42\n",
            "EPOCH: 43\n",
            "EPOCH: 44\n",
            "EPOCH: 45\n",
            "EPOCH: 46\n",
            "EPOCH: 47\n",
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_47.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_47.pt\n",
            "EPOCH: 48\n",
            "EPOCH: 49\n",
            "EPOCH: 50\n",
            "EPOCH: 51\n",
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_51.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_51.pt\n",
            "EPOCH: 52\n",
            "EPOCH: 53\n",
            "EPOCH: 54\n",
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_54.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_54.pt\n",
            "EPOCH: 55\n",
            "EPOCH: 56\n",
            "EPOCH: 57\n",
            "EPOCH: 58\n",
            "EPOCH: 59\n",
            "EPOCH: 60\n",
            "EPOCH: 61\n",
            "EPOCH: 62\n",
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_62.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_62.pt\n",
            "EPOCH: 63\n",
            "EPOCH: 64\n",
            "EPOCH: 65\n",
            "EPOCH: 66\n",
            "EPOCH: 67\n",
            "EPOCH: 68\n",
            "EPOCH: 69\n",
            "EPOCH: 70\n",
            "EPOCH: 71\n",
            "EPOCH: 72\n",
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_72.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_72.pt\n",
            "EPOCH: 73\n",
            "EPOCH: 74\n",
            "EPOCH: 75\n",
            "EPOCH: 76\n",
            "vAE model saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_vAE_epoch_76.pt\n",
            "Discriminator saved at /content/drive/MyDrive/output/Autoencoder_KL/models/trained_discriminator_epoch_76.pt\n",
            "EPOCH: 77\n",
            "EPOCH: 78\n",
            "EPOCH: 79\n",
            "EPOCH: 80\n",
            "EPOCH: 81\n",
            "EPOCH: 82\n",
            "EPOCH: 83\n",
            "EPOCH: 84\n",
            "EPOCH: 85\n",
            "EPOCH: 86\n",
            "EPOCH: 87\n",
            "EPOCH: 88\n",
            "EPOCH: 89\n",
            "EPOCH: 90\n",
            "EPOCH: 91\n",
            "EPOCH: 92\n",
            "EPOCH: 93\n",
            "EPOCH: 94\n",
            "EPOCH: 95\n",
            "EPOCH: 96\n",
            "EPOCH: 97\n",
            "EPOCH: 98\n",
            "EPOCH: 99\n"
          ]
        }
      ],
      "source": [
        "#%% Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#%% Setup paths\n",
        "#Define directories\n",
        "data_dir = os.path.join('/content/', \"training_data\")\n",
        "slices_dir = os.path.join(data_dir, 'slices_40_new_anon')\n",
        "\n",
        "#%% Load CSV\n",
        "data_key = pd.read_csv(os.path.join(data_dir, 'data_key_new_anon.csv'))\n",
        "#data_key = data_key[data_key['Linked_Files_Anon'].apply(lambda x: os.path.exists(os.path.join(slices_dir, x)))]\n",
        "data_key = data_key.reset_index(drop=True)\n",
        "\n",
        "#%% Input arguments (Sets training to true)\n",
        "# Set default values:\n",
        "autoencoder_weights         = os.path.join(base_dir, 'models', 'model_autoencoder.pt')\n",
        "\n",
        "#%% Load vAE wandb config & Setup output directories\n",
        "wandb_config_vAE = load_wandb_config_vAE()\n",
        "#Create output directory\n",
        "output_dir       = os.path.join(base_dir, \"output\", wandb_config_vAE['architecture'])\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# =================================================================================\n",
        "# Setup Datasets and DataLoaders for vAE Training\n",
        "# =================================================================================\n",
        "# Augmentations are applied only to the training set for the vAE\n",
        "aug_transforms = Augmentations()\n",
        "\n",
        "# Create Dataset instances\n",
        "train_dataset,  \\\n",
        "val_dataset,    \\\n",
        "test_dataset    = create_datasets(data_key, aug_transforms, slices_dir)\n",
        "\n",
        "# Create DataLoader instances\n",
        "# Note: shuffle=False for val and test loaders for consistent evaluation\n",
        "train_loader     = DataLoader(train_dataset, batch_size=wandb_config_vAE['batch_size'], shuffle=True)\n",
        "val_loader       = DataLoader(val_dataset, batch_size=wandb_config_vAE['batch_size'], shuffle=False)\n",
        "test_loader      = DataLoader(test_dataset, batch_size=wandb_config_vAE['batch_size'], shuffle=False)\n",
        "\n",
        "#%% Load losses\n",
        "perceptual_loss = load_perceptual_loss(device=device).float()\n",
        "discriminator   = load_discriminator(device=device).float()\n",
        "\n",
        "#%% Load the vAE model and optimizer\n",
        "KL_autoencoder  = load_KL_autoencoder(autoencoder_weights,\n",
        "                                        config_file=\"train_autoencoder.json\",\n",
        "                                        device=device).float()\n",
        "\n",
        "optimizer_g   = optim.Adam(params=list(KL_autoencoder.parameters()), lr=wandb_config_vAE[\"learning_rate\"])\n",
        "optimizer_d   = optim.Adam(params=list(discriminator.parameters()), lr=wandb_config_vAE[\"learning_rate\"])\n",
        "\n",
        "\n",
        "#%% Train the vAE model\n",
        "print(\"Training vAE\")\n",
        "# Pass both training and validation loaders to the training function\n",
        "# You might need to adapt your `train_autoencoder` function to use the validation loader\n",
        "KL_autoencoder = train_autoencoder(KL_autoencoder,\n",
        "                                    discriminator,\n",
        "                                    perceptual_loss,\n",
        "                                    generator_loss,\n",
        "                                    optimizer_g,\n",
        "                                    optimizer_d,\n",
        "                                    train_loader, # Use the vAE specific training loader\n",
        "                                    val_loader,       # Pass validation loader for evaluation\n",
        "                                    output_dir=output_dir,\n",
        "                                    device=device)"
      ]
    }
  ]
}